{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dvmorris/coca-image-embedding/blob/master/Build_Cloud_CoCa_Image_Embedding_Dataset_%26_Search.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Copyright 2023 Google LLC.\n",
        "\n",
        "SPDX-License-Identifier: Apache-2.0"
      ],
      "metadata": {
        "id": "v-kGkghtSidL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ],
      "metadata": {
        "id": "nhRV4YeIScWz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Overview\n",
        "\n",
        "This Colab demonstrates CoCa image embedding (https://arxiv.org/abs/2205.01917) inference end-to-end, including getting embedding from both image and text, indexing the embedding and search using either ScaNN or Vertex Matching Engine.\n",
        "\n",
        "The information in this documentation is provided to the customer on an “as is” and “with all faults” basis without any warranty of any kind, either express or implied. Google does not warrant or guarantee the correctness, accuracy or reliability of the information in here. In no event will Google or its affiliates or licensors be liable for any damage or harm to customers from customer’s use of these materials.\n",
        "\n",
        "Author: wuxiao@google.com"
      ],
      "metadata": {
        "id": "GZ3H2bRfHuYP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Section 0: Preparation\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "This is to set up shared functions and import shared packages for the remaining of the Colab to complete. Please execute this section every time the runtime get resets.\n",
        "\n",
        "Note: The installation may complain some version conflicts. Seems OK to ignore."
      ],
      "metadata": {
        "id": "Czg2D10QmR8l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# does not work in Argolis\n",
        "# !gsutil cp gs://vertex-ai-allowlist/generative-ai/vision/multimodalembedding/requirements.txt .\n",
        "# !pip3 install -r requirements.txt\n",
        "\n",
        "!pip3 install absl-py==1.4.0\n",
        "!pip3 install google-cloud-aiplatform==1.25.0\n",
        "!pip3 install proto-plus==1.22.2\n",
        "!pip3 install protobuf==4.23.0\n",
        "\n",
        "!pip install scann"
      ],
      "metadata": {
        "id": "S0OJMJyW7lzW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "4c9ab846-3a55-4f01-f0e9-a2dc4b22f170"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: absl-py==1.4.0 in /usr/local/lib/python3.10/dist-packages (1.4.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting google-cloud-aiplatform==1.25.0\n",
            "  Downloading google_cloud_aiplatform-1.25.0-py2.py3-none-any.whl (2.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m62.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform==1.25.0) (2.11.0)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform==1.25.0) (1.22.2)\n",
            "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform==1.25.0) (3.20.3)\n",
            "Requirement already satisfied: packaging>=14.3 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform==1.25.0) (23.1)\n",
            "Requirement already satisfied: google-cloud-storage<3.0.0dev,>=1.32.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform==1.25.0) (2.8.0)\n",
            "Requirement already satisfied: google-cloud-bigquery<4.0.0dev,>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform==1.25.0) (3.9.0)\n",
            "Collecting google-cloud-resource-manager<3.0.0dev,>=1.3.3 (from google-cloud-aiplatform==1.25.0)\n",
            "  Downloading google_cloud_resource_manager-1.10.1-py2.py3-none-any.whl (321 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m321.3/321.3 kB\u001b[0m \u001b[31m29.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting shapely<2.0.0 (from google-cloud-aiplatform==1.25.0)\n",
            "  Downloading Shapely-1.8.5.post1-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m68.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: googleapis-common-protos<2.0dev,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform==1.25.0) (1.59.0)\n",
            "Requirement already satisfied: google-auth<3.0dev,>=2.14.1 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform==1.25.0) (2.17.3)\n",
            "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform==1.25.0) (2.27.1)\n",
            "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform==1.25.0) (1.54.0)\n",
            "Requirement already satisfied: grpcio-status<2.0dev,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform==1.25.0) (1.48.2)\n",
            "Requirement already satisfied: google-cloud-core<3.0.0dev,>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-bigquery<4.0.0dev,>=1.15.0->google-cloud-aiplatform==1.25.0) (2.3.2)\n",
            "Requirement already satisfied: google-resumable-media<3.0dev,>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-bigquery<4.0.0dev,>=1.15.0->google-cloud-aiplatform==1.25.0) (2.5.0)\n",
            "Requirement already satisfied: python-dateutil<3.0dev,>=2.7.2 in /usr/local/lib/python3.10/dist-packages (from google-cloud-bigquery<4.0.0dev,>=1.15.0->google-cloud-aiplatform==1.25.0) (2.8.2)\n",
            "Collecting grpc-google-iam-v1<1.0.0dev,>=0.12.4 (from google-cloud-resource-manager<3.0.0dev,>=1.3.3->google-cloud-aiplatform==1.25.0)\n",
            "  Downloading grpc_google_iam_v1-0.12.6-py2.py3-none-any.whl (26 kB)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0dev,>=2.14.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform==1.25.0) (5.3.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0dev,>=2.14.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform==1.25.0) (0.3.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0dev,>=2.14.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform==1.25.0) (1.16.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0dev,>=2.14.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform==1.25.0) (4.9)\n",
            "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /usr/local/lib/python3.10/dist-packages (from google-resumable-media<3.0dev,>=0.6.0->google-cloud-bigquery<4.0.0dev,>=1.15.0->google-cloud-aiplatform==1.25.0) (1.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform==1.25.0) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform==1.25.0) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform==1.25.0) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform==1.25.0) (3.4)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0dev,>=2.14.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform==1.25.0) (0.5.0)\n",
            "Installing collected packages: shapely, grpc-google-iam-v1, google-cloud-resource-manager, google-cloud-aiplatform\n",
            "  Attempting uninstall: shapely\n",
            "    Found existing installation: shapely 2.0.1\n",
            "    Uninstalling shapely-2.0.1:\n",
            "      Successfully uninstalled shapely-2.0.1\n",
            "Successfully installed google-cloud-aiplatform-1.25.0 google-cloud-resource-manager-1.10.1 grpc-google-iam-v1-0.12.6 shapely-1.8.5.post1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: proto-plus==1.22.2 in /usr/local/lib/python3.10/dist-packages (1.22.2)\n",
            "Requirement already satisfied: protobuf<5.0.0dev,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from proto-plus==1.22.2) (3.20.3)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting protobuf==4.23.0\n",
            "  Downloading protobuf-4.23.0-cp37-abi3-manylinux2014_x86_64.whl (304 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m304.5/304.5 kB\u001b[0m \u001b[31m20.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: protobuf\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 3.20.3\n",
            "    Uninstalling protobuf-3.20.3:\n",
            "      Successfully uninstalled protobuf-3.20.3\n",
            "Successfully installed protobuf-4.23.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting scann\n",
            "  Downloading scann-1.2.9-cp310-cp310-manylinux_2_27_x86_64.whl (10.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.5/10.5 MB\u001b[0m \u001b[31m82.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tensorflow~=2.11.0 (from scann)\n",
            "  Downloading tensorflow-2.11.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (588.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m588.3/588.3 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from scann) (1.22.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from scann) (1.16.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.11.0->scann) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.11.0->scann) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.11.0->scann) (23.3.3)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.11.0->scann) (0.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.11.0->scann) (0.2.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.11.0->scann) (1.54.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.11.0->scann) (3.8.0)\n",
            "Collecting keras<2.12,>=2.11.0 (from tensorflow~=2.11.0->scann)\n",
            "  Downloading keras-2.11.0-py2.py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m82.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.11.0->scann) (16.0.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.11.0->scann) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.11.0->scann) (23.1)\n",
            "Collecting protobuf<3.20,>=3.9.2 (from tensorflow~=2.11.0->scann)\n",
            "  Downloading protobuf-3.19.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m77.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.11.0->scann) (67.7.2)\n",
            "Collecting tensorboard<2.12,>=2.11 (from tensorflow~=2.11.0->scann)\n",
            "  Downloading tensorboard-2.11.2-py3-none-any.whl (6.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m106.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tensorflow-estimator<2.12,>=2.11.0 (from tensorflow~=2.11.0->scann)\n",
            "  Downloading tensorflow_estimator-2.11.0-py2.py3-none-any.whl (439 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m439.2/439.2 kB\u001b[0m \u001b[31m44.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.11.0->scann) (2.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.11.0->scann) (4.5.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.11.0->scann) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.11.0->scann) (0.32.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow~=2.11.0->scann) (0.40.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.12,>=2.11->tensorflow~=2.11.0->scann) (2.17.3)\n",
            "Collecting google-auth-oauthlib<0.5,>=0.4.1 (from tensorboard<2.12,>=2.11->tensorflow~=2.11.0->scann)\n",
            "  Downloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.12,>=2.11->tensorflow~=2.11.0->scann) (3.4.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.12,>=2.11->tensorflow~=2.11.0->scann) (2.27.1)\n",
            "Collecting tensorboard-data-server<0.7.0,>=0.6.0 (from tensorboard<2.12,>=2.11->tensorflow~=2.11.0->scann)\n",
            "  Downloading tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl (4.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m113.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.12,>=2.11->tensorflow~=2.11.0->scann) (1.8.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.12,>=2.11->tensorflow~=2.11.0->scann) (2.3.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow~=2.11.0->scann) (5.3.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow~=2.11.0->scann) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow~=2.11.0->scann) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow~=2.11.0->scann) (1.3.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow~=2.11.0->scann) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow~=2.11.0->scann) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow~=2.11.0->scann) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow~=2.11.0->scann) (3.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.12,>=2.11->tensorflow~=2.11.0->scann) (2.1.2)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow~=2.11.0->scann) (0.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow~=2.11.0->scann) (3.2.2)\n",
            "Installing collected packages: tensorflow-estimator, tensorboard-data-server, protobuf, keras, google-auth-oauthlib, tensorboard, tensorflow, scann\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.12.0\n",
            "    Uninstalling tensorflow-estimator-2.12.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.12.0\n",
            "  Attempting uninstall: tensorboard-data-server\n",
            "    Found existing installation: tensorboard-data-server 0.7.0\n",
            "    Uninstalling tensorboard-data-server-0.7.0:\n",
            "      Successfully uninstalled tensorboard-data-server-0.7.0\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 4.23.0\n",
            "    Uninstalling protobuf-4.23.0:\n",
            "      Successfully uninstalled protobuf-4.23.0\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 2.12.0\n",
            "    Uninstalling keras-2.12.0:\n",
            "      Successfully uninstalled keras-2.12.0\n",
            "  Attempting uninstall: google-auth-oauthlib\n",
            "    Found existing installation: google-auth-oauthlib 1.0.0\n",
            "    Uninstalling google-auth-oauthlib-1.0.0:\n",
            "      Successfully uninstalled google-auth-oauthlib-1.0.0\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.12.2\n",
            "    Uninstalling tensorboard-2.12.2:\n",
            "      Successfully uninstalled tensorboard-2.12.2\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.12.0\n",
            "    Uninstalling tensorflow-2.12.0:\n",
            "      Successfully uninstalled tensorflow-2.12.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow-datasets 4.9.2 requires protobuf>=3.20, but you have protobuf 3.19.6 which is incompatible.\n",
            "tensorflow-metadata 1.13.1 requires protobuf<5,>=3.20.3, but you have protobuf 3.19.6 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed google-auth-oauthlib-0.4.6 keras-2.11.0 protobuf-3.19.6 scann-1.2.9 tensorboard-2.11.2 tensorboard-data-server-0.6.1 tensorflow-2.11.1 tensorflow-estimator-2.11.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 0.0: Define embedding calculation functions. Both image embedding and text embedding calculators are defined below."
      ],
      "metadata": {
        "id": "V8W_7GIanteW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The embedding extraction is served from Vertex AI. Please enable that at your Cloud Console or run `gcloud services enable aiplatform.googleapis.com --project {your_project}`.\n",
        "\n",
        "Use of the API requires allowlisting. Please follow the Prerequisite section in https://docs.google.com/document/d/1UdJSN3I1XfF97g1Ocukse89eaMSksVXGzfFjximPKgs/edit#heading=h.d8qh4jlew6dg to get allowlisted."
      ],
      "metadata": {
        "id": "LvCkwCrQCNuT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import base64\n",
        "from google.cloud import storage\n",
        "from google.cloud import aiplatform\n",
        "from google.protobuf import struct_pb2\n",
        "import sys\n",
        "import time\n",
        "import typing\n",
        "\n",
        "PROJECT_ID = 'gen-ai-demo-davemorris-2023' # @param {type: \"string\"}\n",
        "\n",
        "# Inspired from https://stackoverflow.com/questions/34269772/type-hints-in-namedtuple.\n",
        "class EmbeddingResponse(typing.NamedTuple):\n",
        "  text_embedding: typing.Sequence[float]\n",
        "  image_embedding: typing.Sequence[float]\n",
        "\n",
        "class EmbeddingPredictionClient:\n",
        "  \"\"\"Wrapper around Prediction Service Client.\"\"\"\n",
        "  def __init__(self, project : str,\n",
        "    location : str = \"us-central1\",\n",
        "    api_regional_endpoint: str = \"us-central1-aiplatform.googleapis.com\"):\n",
        "    client_options = {\"api_endpoint\": api_regional_endpoint}\n",
        "    # Initialize client that will be used to create and send requests.\n",
        "    # This client only needs to be created once, and can be reused for multiple requests.\n",
        "    self.client = aiplatform.gapic.PredictionServiceClient(client_options=client_options)\n",
        "    self.location = location\n",
        "    self.project = project\n",
        "\n",
        "  def get_embedding(self, text : str = None, image_bytes : bytes = None):\n",
        "    if not text and not image_bytes:\n",
        "      raise ValueError('At least one of text or image_bytes must be specified.')\n",
        "\n",
        "    instance = struct_pb2.Struct()\n",
        "    if text:\n",
        "      instance.fields['text'].string_value = text\n",
        "\n",
        "    if image_bytes:\n",
        "      encoded_content = base64.b64encode(image_bytes).decode(\"utf-8\")\n",
        "      image_struct = instance.fields['image'].struct_value\n",
        "      image_struct.fields['bytesBase64Encoded'].string_value = encoded_content\n",
        "\n",
        "    instances = [instance]\n",
        "    endpoint = (f\"projects/{self.project}/locations/{self.location}\"\n",
        "      \"/publishers/google/models/multimodalembedding@001\")\n",
        "    response = self.client.predict(endpoint=endpoint, instances=instances)\n",
        "\n",
        "    text_embedding = None\n",
        "    if text:\n",
        "      text_emb_value = response.predictions[0]['textEmbedding']\n",
        "      text_embedding = [v for v in text_emb_value]\n",
        "\n",
        "    image_embedding = None\n",
        "    if image_bytes:\n",
        "      image_emb_value = response.predictions[0]['imageEmbedding']\n",
        "      image_embedding = [v for v in image_emb_value]\n",
        "\n",
        "    return EmbeddingResponse(\n",
        "      text_embedding=text_embedding,\n",
        "      image_embedding=image_embedding)\n",
        "\n",
        "client = EmbeddingPredictionClient(project=PROJECT_ID)\n",
        "\n",
        "# Extract image embedding\n",
        "def getImageEmbeddingFromImageContent(content):\n",
        "  response = client.get_embedding(text=None, image_bytes=content)\n",
        "  return response.image_embedding\n",
        "\n",
        "def getImageEmbeddingFromGcsObject(gcsBucket, gcsObject):\n",
        "  client = storage.Client()\n",
        "  bucket = client.bucket(gcsBucket)\n",
        "  blob = bucket.blob(gcsObject)\n",
        "\n",
        "  with blob.open(\"rb\") as f:\n",
        "    return getImageEmbeddingFromImageContent(f.read())\n",
        "\n",
        "def getTextEmbeddingFromGcsObject(gcsBucket, gcsObject):\n",
        "  client = storage.Client()\n",
        "  bucket = client.bucket(gcsBucket)\n",
        "  blob = bucket.blob(gcsObject)\n",
        "\n",
        "  with blob.open(\"r\") as f:\n",
        "    return getTextEmbedding(f.read())\n",
        "\n",
        "def getImageEmbeddingFromFile(filePath):\n",
        "  with open(filePath, \"rb\") as f:\n",
        "    return getImageEmbeddingFromImageContent(f.read())\n",
        "\n",
        "# Extract text embedding\n",
        "def getTextEmbedding(text):\n",
        "  response = client.get_embedding(text=text, image_bytes=None)\n",
        "  return response.text_embedding\n"
      ],
      "metadata": {
        "id": "JDgD0lsopevS"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 0.1: Authenticate yourself.\n",
        "\n",
        "As downstream references GCP services (e.g. GCS) in almost all sections, please get yourself authenticated so you can access the services without getting 403 (Access Denied)."
      ],
      "metadata": {
        "id": "UgX-mNUKn778"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import auth as google_auth\n",
        "\n",
        "google_auth.authenticate_user()"
      ],
      "metadata": {
        "id": "0kUvlVb1HvVW"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Section 1: Build CoCa image embedding dataset.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "This section calculates the CoCa image embeddings for a set of images stored in GCS. The number of images in the dataset is less than 100 so we are calculating the embedding in sequence. For larger datasets, please use Dataflow to improve the parallelism.\n",
        "\n",
        "The image dataset is contributed by the engineering team."
      ],
      "metadata": {
        "id": "AVSEmCFHohG2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.cloud import storage\n",
        "import csv\n",
        "import time\n",
        "# This is the GCS bucket that holds the images that you want to analyze and\n",
        "# index. You will need the bucket list and object reading permission to proceed.\n",
        "# The default bucket provided here contains 61 images contributed by the\n",
        "# engineer team. If you want to try your own image set, feel free to point this\n",
        "# to another GCS bucket that holds your images. Please make sure all files in\n",
        "# the GCS bucket are images (e.g. JPG, PNG). Non image files would cause\n",
        "# inference exception down below.\n",
        "IMAGE_SET_BUCKET_NAME = \"gen-ai-demo-davemorris-2023\" # @param {type: \"string\"}\n",
        "\n",
        "gcsBucket = storage.Client().get_bucket(IMAGE_SET_BUCKET_NAME)\n",
        "ids = []\n",
        "\n",
        "with open('image_embedding.csv', 'r') as f:\n",
        "  reader = csv.reader(f, delimiter=',')\n",
        "  for row in reader:\n",
        "    ids.append(row[0].split('/')[-1].split('.')[0])\n",
        "\n",
        "with open('image_embedding.csv', 'a') as f:\n",
        "  csvWriter = csv.writer(f)\n",
        "  csvWriter.writerow(['gcsUri', 'embedding'])\n",
        "  prefix='grocery-product-images/group1/'\n",
        "  for blob in gcsBucket.list_blobs(prefix=prefix):\n",
        "    if blob.name.split('/')[-1].split('.')[0] not in ids:\n",
        "      gcsUri = \"gs://\" + IMAGE_SET_BUCKET_NAME + prefix + blob.name\n",
        "      print(\"Processing {}\".format(gcsUri))\n",
        "      embedding = getImageEmbeddingFromGcsObject(IMAGE_SET_BUCKET_NAME, blob.name)\n",
        "      time.sleep(1)\n",
        "      csvWriter.writerow([gcsUri, str(embedding)])"
      ],
      "metadata": {
        "id": "x2BrVlM-phGN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "48563974-1fe2-4b77-c4d5-0c9be8c344fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing gs://gen-ai-demo-davemorris-2023grocery-product-images/group1/grocery-product-images/group1/129104.jpg\n",
            "Processing gs://gen-ai-demo-davemorris-2023grocery-product-images/group1/grocery-product-images/group1/129108.jpg\n",
            "Processing gs://gen-ai-demo-davemorris-2023grocery-product-images/group1/grocery-product-images/group1/129109.jpg\n",
            "Processing gs://gen-ai-demo-davemorris-2023grocery-product-images/group1/grocery-product-images/group1/129110.jpg\n",
            "Processing gs://gen-ai-demo-davemorris-2023grocery-product-images/group1/grocery-product-images/group1/129113.jpg\n",
            "Processing gs://gen-ai-demo-davemorris-2023grocery-product-images/group1/grocery-product-images/group1/129114.jpg\n",
            "Processing gs://gen-ai-demo-davemorris-2023grocery-product-images/group1/grocery-product-images/group1/129116.jpg\n",
            "Processing gs://gen-ai-demo-davemorris-2023grocery-product-images/group1/grocery-product-images/group1/129117.jpg\n",
            "Processing gs://gen-ai-demo-davemorris-2023grocery-product-images/group1/grocery-product-images/group1/129118.jpg\n",
            "Processing gs://gen-ai-demo-davemorris-2023grocery-product-images/group1/grocery-product-images/group1/129119.jpg\n",
            "Processing gs://gen-ai-demo-davemorris-2023grocery-product-images/group1/grocery-product-images/group1/129120.jpg\n",
            "Processing gs://gen-ai-demo-davemorris-2023grocery-product-images/group1/grocery-product-images/group1/129125.jpg\n",
            "Processing gs://gen-ai-demo-davemorris-2023grocery-product-images/group1/grocery-product-images/group1/129126.jpg\n",
            "Processing gs://gen-ai-demo-davemorris-2023grocery-product-images/group1/grocery-product-images/group1/129127.jpg\n",
            "Processing gs://gen-ai-demo-davemorris-2023grocery-product-images/group1/grocery-product-images/group1/129129.jpg\n",
            "Processing gs://gen-ai-demo-davemorris-2023grocery-product-images/group1/grocery-product-images/group1/129133.jpg\n",
            "Processing gs://gen-ai-demo-davemorris-2023grocery-product-images/group1/grocery-product-images/group1/129135.jpg\n",
            "Processing gs://gen-ai-demo-davemorris-2023grocery-product-images/group1/grocery-product-images/group1/129136.jpg\n",
            "Processing gs://gen-ai-demo-davemorris-2023grocery-product-images/group1/grocery-product-images/group1/129137.jpg\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As the image embedding calculation can take long, we can save the generated image_embedding.csv locally so we can start section 2 later on in a separate session."
      ],
      "metadata": {
        "id": "AWXlKvefpSUV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.cloud import storage\n",
        "import csv\n",
        "import time\n",
        "# This is the GCS bucket that holds the text files that you want to analyze and\n",
        "# index. You will need the bucket list and object reading permission to proceed.\n",
        "TEXT_SET_BUCKET_NAME = \"gen-ai-demo-davemorris-2023\" # @param {type: \"string\"}\n",
        "\n",
        "gcsBucket = storage.Client().get_bucket(TEXT_SET_BUCKET_NAME)\n",
        "ids = []\n",
        "\n",
        "with open('text_embedding.csv', 'r') as f:\n",
        "  reader = csv.reader(f, delimiter=',')\n",
        "  for row in reader:\n",
        "    ids.append(row[0].split('/')[-1].split('.')[0])\n",
        "\n",
        "with open('text_embedding.csv', 'a') as f:\n",
        "  csvWriter = csv.writer(f)\n",
        "  csvWriter.writerow(['gcsUri', 'embedding'])\n",
        "  prefix='grocery-product-titles/group1/'\n",
        "  for blob in gcsBucket.list_blobs(prefix=prefix):\n",
        "    if blob.name.split('/')[-1].split('.')[0] not in ids:\n",
        "      gcsUri = \"gs://\" + TEXT_SET_BUCKET_NAME + '/' + prefix + blob.name\n",
        "      print(\"Processing {}\".format(gcsUri))\n",
        "      embedding = getTextEmbeddingFromGcsObject(TEXT_SET_BUCKET_NAME, blob.name)\n",
        "      time.sleep(1)\n",
        "      csvWriter.writerow([gcsUri, str(embedding)])"
      ],
      "metadata": {
        "id": "hmN3EjD97o6g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "files.download('image_embedding.csv')\n",
        "files.download('text_embedding.csv')"
      ],
      "metadata": {
        "id": "nsU8zmmnfchx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Section 2: Zero shot image search by text or image similarity.\n",
        "\n",
        "---\n",
        "This section searches the images by either ScaNN or Vertex Matching Engine. Text embedding is calculated using getTextEmbedding() defined in Section 0.\n",
        "Image embedding is calculated using getImageEmbeddingFromFile() defined in Section 0 too.\n"
      ],
      "metadata": {
        "id": "Uc5SLuEuqQc8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Section 2.0: Preparation\n",
        "\n",
        "### Step 2.0.0: Upload the image_embedding.csv from where you uploaded to at the end of section 1.\n",
        "\n",
        "If you are directly doing after section 1 without restarting the runtime, the file existance check will skip the file upload automatically."
      ],
      "metadata": {
        "id": "730svcR_utRD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "import os\n",
        "\n",
        "if not os.path.exists('image_embedding.csv'):\n",
        "  print(\"Upload your saved image_embedding.csv\")\n",
        "\n",
        "  uploaded = files.upload()\n",
        "\n",
        "  found_file = False\n",
        "  for filename in uploaded.keys():\n",
        "    print(uploaded)\n",
        "    if filename == \"image_embedding.csv\":\n",
        "      print(\"Found your image_embedding.csv\")\n",
        "      found_file = True\n",
        "\n",
        "  if not found_file:\n",
        "    raise ValueError(\"No image_embedding.csv uploaded\")"
      ],
      "metadata": {
        "id": "FZxvruXrRGQ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load the image embedding file to pandas."
      ],
      "metadata": {
        "id": "MBICZK4nvDJU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "df = pd.read_csv('image_embedding.csv')\n",
        "df.embedding = df.embedding.apply(eval).apply(np.array)\n",
        "\n",
        "df.head(5)"
      ],
      "metadata": {
        "id": "OO6pEaxlk-SF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 2.0.1: Define a helper function to search image by text.\n",
        "\n",
        "It does the following in order:\n",
        "\n",
        "1. calculate the text embedding\n",
        "2. search the embedding from the search backend (ScaNN or VME), and get top 3 closest neighbors.\n",
        "3. Display the search results and the corresponding images."
      ],
      "metadata": {
        "id": "8UzRSam0w1BP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pandas.io.parsers.readers import ParserBase\n",
        "import time\n",
        "import re\n",
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "from google.colab import files\n",
        "\n",
        "# @@search_backend_function is a function that takes two parameters\n",
        "#   @@embedding_vector: a embedding vector to search against.\n",
        "#   @@num_neighbors: number of neighbors to return from the backend.\n",
        "# and it returns two things\n",
        "#   @@neighbors: a list of ids (0 based position in the dataset) indicating the\n",
        "#                neighbors that's closest to the input embedding_vector.\n",
        "#   @@distances: a list of distances, each corresponding to the distance from\n",
        "#                the @@embedding_vector to the data point int he dataset,\n",
        "#                indexed by the corresponding id in @@neighbor.\n",
        "def searchImagesByEmbedding(start_time, embedding, search_backend_function,\n",
        "                            num_neighbors = 3):\n",
        "    neighbors, distances = search_backend_function(\n",
        "        embedding, num_neighbors)\n",
        "    end = time.time()\n",
        "\n",
        "    gcsClient = storage.Client()\n",
        "    for id, dist in zip(neighbors, distances):\n",
        "        print(f'docid:{id} dist:{dist} gcsUri:{df.gcsUri[id]}')\n",
        "        # Display the image\n",
        "        gcsUri = df.gcsUri[id]\n",
        "        m = re.search('gs://([^/]*)/([^$]*)', gcsUri)\n",
        "        imageBlob = gcsClient.get_bucket(m[1]).blob(m[2])\n",
        "        tmpFilename = \"/tmp/tmp_image\"\n",
        "        imageBlob.download_to_filename(tmpFilename)\n",
        "        image = cv2.imread(tmpFilename, -1)\n",
        "        cv2_imshow(image)\n",
        "\n",
        "    print(\"Latency (ms):\", 1000*(end - start_time))\n",
        "\n",
        "def searchImagesByText(query, search_backend_function, num_neighbors = 3):\n",
        "    start_time = time.time()\n",
        "    query_embedding = getTextEmbedding(query)\n",
        "    return searchImagesByEmbedding(start_time, query_embedding,\n",
        "                                   search_backend_function)\n",
        "\n",
        "def searchImagesByUploadedImages(search_backend_function, num_neighbors = 3):\n",
        "    uploaded = files.upload()\n",
        "    for filename in uploaded.keys():\n",
        "      print('Searching images similar to {}'.format(filename))\n",
        "      image = cv2.imread(filename, -1)\n",
        "      cv2_imshow(image)\n",
        "      start_time = time.time()\n",
        "      image_embedding = getImageEmbeddingFromFile(filename)\n",
        "      searchImagesByEmbedding(start_time, image_embedding,\n",
        "                              search_backend_function, num_neighbors)\n"
      ],
      "metadata": {
        "id": "BjPpUDM4xax-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Section 2.1 and Section 2.2 shows how to search using ScaNN and VME respectively. You can try one of them, or both. If you want to try search using VME directly, please jump to Section 2.2."
      ],
      "metadata": {
        "id": "Vel7cDAgvTCB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Section 2.1: Search using ScaNN."
      ],
      "metadata": {
        "id": "QyQvAo3HvsgA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 2.1.0: Construct the dataset that ScaNN consumes as the input. And then define the function to search against it."
      ],
      "metadata": {
        "id": "QrzBk-lhSwAN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import scann\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# df.shape[0] is the #data in the dataset.\n",
        "# df.embedding[0].size is the embedding vector size.\n",
        "dataset = np.empty((df.shape[0], df.embedding[0].size))\n",
        "for i in range(df.shape[0]):\n",
        "  dataset[i] = df.embedding[i]\n",
        "\n",
        "searcher = scann.scann_ops_pybind.builder(dataset, 10, \"dot_product\").tree(\n",
        "    num_leaves=10, num_leaves_to_search=10).score_ah(2).reorder(100).build()\n",
        "\n",
        "def searchByScaNN(embedding_vector, num_neighbors):\n",
        "    return searcher.search(\n",
        "        embedding_vector, final_num_neighbors = num_neighbors)"
      ],
      "metadata": {
        "id": "n9IzB6NRl9Ts"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 2.1.1: Search\n",
        "\n",
        "You can either do search by text, or search by image. Search by image is not the advantages of CoCa model, so here just shows you its capability."
      ],
      "metadata": {
        "id": "8TfqtpChTLIW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Search by text. Modify the query and execute to see the search result.\n",
        "searchImagesByText(\"lake view\", searchByScaNN)"
      ],
      "metadata": {
        "id": "DsFffqTCyBv9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Search by image. Upload your own image and search.\n",
        "searchImagesByUploadedImages(searchByScaNN)"
      ],
      "metadata": {
        "id": "9ISjkV08Dvta"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Section 2.2: Search using Vertex Matching Engine.\n",
        "\n",
        "Please note that using Vertex Matching Engine for embedding search would incur cost to your GCP project."
      ],
      "metadata": {
        "id": "1Ub7V-4L2dxb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 2.2.0: Define a set of constants used by downstream."
      ],
      "metadata": {
        "id": "IZ-kRu9F6KT_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "REGION = 'us-central1' # @param {type: \"string\"}\n",
        "\n",
        "! gcloud config set project {PROJECT_ID}\n",
        "\n",
        "ENDPOINT = \"{}-aiplatform.googleapis.com\".format(REGION)\n",
        "\n",
        "PROJECT_NUMBER = !gcloud projects list --filter=\"PROJECT_ID:'{PROJECT_ID}'\" --format='value(PROJECT_NUMBER)'\n",
        "PROJECT_NUMBER = PROJECT_NUMBER[0]\n",
        "\n",
        "PARENT = \"projects/{}/locations/{}\".format(PROJECT_ID, REGION)\n",
        "\n",
        "# Bucket for holding the indexing data for VME. You will need the bucket\n",
        "# writing permission to proceed.\n",
        "INDEX_DATA_BUCKET_NAME = '' # @param {type: \"string\"}\n",
        "\n",
        "! gsutil mb -l $REGION -p $PROJECT_ID gs://{INDEX_DATA_BUCKET_NAME}\n",
        "! gsutil rm -raf gs://{INDEX_DATA_BUCKET_NAME}/** 2> /dev/null || true"
      ],
      "metadata": {
        "id": "vHa0mP4x6QWv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 2.2.1: Convert the image embedding into the JSON format that VME recognizes.\n",
        "\n",
        "The idx is the position of the data point in the dataset.\n",
        "\n",
        "After conversion, upload the JSON file to a empty GCS bucket so VME can consume."
      ],
      "metadata": {
        "id": "DPWyDSm94OtN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "with open('image_embedding.json', 'w') as f:\n",
        "  for idx, embedding in enumerate(df.embedding):\n",
        "    json.dump({\"id\": idx, \"embedding\": embedding.tolist()}, f)\n",
        "    f.write('\\n')\n",
        "\n",
        "! gsutil cp image_embedding.json gs://{INDEX_DATA_BUCKET_NAME}/image_embedding.json"
      ],
      "metadata": {
        "id": "JBF8j8NX4jhx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 2.2.2: Create VME index from the dataset. The index creation may take ~30 minutes."
      ],
      "metadata": {
        "id": "mIfWkRai7FFg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "from google.cloud import aiplatform_v1beta1\n",
        "from google.protobuf import struct_pb2\n",
        "\n",
        "DIMENSIONS = df.embedding[0].size\n",
        "DISPLAY_NAME = \"image-embedding\"\n",
        "\n",
        "index_client = aiplatform_v1beta1.IndexServiceClient(\n",
        "    client_options=dict(api_endpoint=ENDPOINT)\n",
        ")\n",
        "\n",
        "treeAhConfig = struct_pb2.Struct(\n",
        "    fields={\n",
        "        \"leafNodeEmbeddingCount\": struct_pb2.Value(number_value=df.shape[0]),\n",
        "        \"leafNodesToSearchPercent\": struct_pb2.Value(number_value=7),\n",
        "    }\n",
        ")\n",
        "\n",
        "algorithmConfig = struct_pb2.Struct(\n",
        "    fields={\"treeAhConfig\": struct_pb2.Value(struct_value=treeAhConfig)}\n",
        ")\n",
        "\n",
        "config = struct_pb2.Struct(\n",
        "    fields={\n",
        "        \"dimensions\": struct_pb2.Value(number_value=DIMENSIONS),\n",
        "        \"approximateNeighborsCount\": struct_pb2.Value(number_value=10),\n",
        "        \"distanceMeasureType\": struct_pb2.Value(string_value=\"DOT_PRODUCT_DISTANCE\"),\n",
        "        \"algorithmConfig\": struct_pb2.Value(struct_value=algorithmConfig),\n",
        "    }\n",
        ")\n",
        "\n",
        "metadata = struct_pb2.Struct(\n",
        "    fields={\n",
        "        \"config\": struct_pb2.Value(struct_value=config),\n",
        "        \"contentsDeltaUri\": struct_pb2.Value(string_value=\"gs://{}\".format(INDEX_DATA_BUCKET_NAME)),\n",
        "    }\n",
        ")\n",
        "\n",
        "matching_engine_index = {\n",
        "    \"display_name\": DISPLAY_NAME,\n",
        "    \"description\": \"Google Products Vertex AI Matching Engine Index\",\n",
        "    \"metadata\": struct_pb2.Value(struct_value=metadata),\n",
        "}\n",
        "\n",
        "# Create the index and wait for it to be ready.\n",
        "matching_engine_index_operation = index_client.create_index(\n",
        "    parent=PARENT, index=matching_engine_index\n",
        ")\n",
        "\n",
        "while not matching_engine_index_operation.done():\n",
        "    print(\"Poll the operation to create index...\")\n",
        "    time.sleep(60)\n",
        "\n",
        "INDEX_RESOURCE_NAME = matching_engine_index_operation.result().name\n",
        "print(\"Index created: {}\".format(INDEX_RESOURCE_NAME))"
      ],
      "metadata": {
        "id": "9NUqOAIC7Hiz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 2.2.3: Create Index Endpoint.\n",
        "\n",
        "Create an index endpoint so that we can deploy the index to. This usually takes a few minutes."
      ],
      "metadata": {
        "id": "1-VyV61k8CK7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "index_endpoint_client = aiplatform_v1beta1.IndexEndpointServiceClient(\n",
        "    client_options=dict(api_endpoint=ENDPOINT)\n",
        ")\n",
        "\n",
        "index_endpoint = {\n",
        "    \"display_name\": \"coca_image_index_endpoint\",\n",
        "    \"public_endpoint_enabled\": True,\n",
        "}\n",
        "\n",
        "index_endpoint_operation = index_endpoint_client.create_index_endpoint(\n",
        "    parent=PARENT, index_endpoint=index_endpoint\n",
        ")\n",
        "\n",
        "while not index_endpoint_operation.done():\n",
        "    print(\"Poll the operation to create index endpoint...\")\n",
        "    time.sleep(60)\n",
        "\n",
        "INDEX_ENDPOINT_NAME = index_endpoint_operation.result().name\n",
        "print(\"Index endpoint created: {}\".format(INDEX_ENDPOINT_NAME))\n",
        "\n",
        "index_endpoint = index_endpoint_client.get_index_endpoint(\n",
        "    name = INDEX_ENDPOINT_NAME)\n",
        "\n",
        "INDEX_ENDPOINT_PUBLIC_DOMAIN_NAME = index_endpoint.public_endpoint_domain_name\n",
        "\n",
        "print(\"Index endpoint public domain name: {}\".format(\n",
        "    INDEX_ENDPOINT_PUBLIC_DOMAIN_NAME))"
      ],
      "metadata": {
        "id": "pDw391nk8PPZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 2.2.4: Deploy the index to the endpoint.\n",
        "\n",
        "This usually takes ~15 minutes."
      ],
      "metadata": {
        "id": "Sj1wRMYM8yjF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DEPLOYED_INDEX_ID = \"coca_image_embedding_deployment\" # @param {type: \"string\"}\n",
        "\n",
        "deploy_matching_engine_index = {\n",
        "    \"id\": DEPLOYED_INDEX_ID,\n",
        "    \"display_name\": DEPLOYED_INDEX_ID,\n",
        "    \"index\": INDEX_RESOURCE_NAME,\n",
        "}\n",
        "\n",
        "deploy_index_operation = index_endpoint_client.deploy_index(\n",
        "    index_endpoint=INDEX_ENDPOINT_NAME, deployed_index=deploy_matching_engine_index\n",
        ")\n",
        "\n",
        "while not deploy_index_operation.done():\n",
        "    print(\"Poll the operation to deploy index...\")\n",
        "    time.sleep(60)\n",
        "\n",
        "deploy_index_operation.result()"
      ],
      "metadata": {
        "id": "c3H5C22V9CDR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 2.2.5: Define the function that searches against VME."
      ],
      "metadata": {
        "id": "iue1SE6hFdyU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from proto.fields import RepeatedField, ProtoType\n",
        "from google.cloud.aiplatform_v1beta1 import MatchServiceClient, IndexDatapoint, FindNeighborsRequest, FindNeighborsResponse\n",
        "\n",
        "match_service_client = MatchServiceClient(\n",
        "    client_options=dict(api_endpoint=INDEX_ENDPOINT_PUBLIC_DOMAIN_NAME)\n",
        ")\n",
        "\n",
        "def searchByVertexMatchingEngine(embedding_to_search, neighbor_count):\n",
        "  datapoint = IndexDatapoint(\n",
        "      feature_vector = embedding_to_search\n",
        "  )\n",
        "\n",
        "  query = FindNeighborsRequest.Query(\n",
        "      datapoint = datapoint,\n",
        "      neighbor_count = neighbor_count\n",
        "  )\n",
        "\n",
        "  find_neighbors_request = FindNeighborsRequest(\n",
        "      index_endpoint = INDEX_ENDPOINT_NAME,\n",
        "      deployed_index_id = DEPLOYED_INDEX_ID,\n",
        "      queries = [query],\n",
        "  )\n",
        "\n",
        "  response = match_service_client.find_neighbors(\n",
        "      request = find_neighbors_request)\n",
        "\n",
        "  neighbors = []\n",
        "  distances = []\n",
        "  for neighbor in response.nearest_neighbors[0].neighbors:\n",
        "    neighbors.append(int(neighbor.datapoint.datapoint_id))\n",
        "    distances.append(neighbor.distance)\n",
        "\n",
        "  return neighbors, distances"
      ],
      "metadata": {
        "id": "g34EblPE9VkS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### Step 2.2.8: Search\n",
        "\n",
        "You can either do search by text, or search by image. Search by image is not the advantages of CoCa model, so here just shows you its capability."
      ],
      "metadata": {
        "id": "XYlYWlD3D_Zn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Search by text. Modify the query and execute to see the search result.\n",
        "searchImagesByText(\"lake view\", searchByVertexMatchingEngine)"
      ],
      "metadata": {
        "id": "waFTzu1u9rVw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Search by image. Upload your own image and search.\n",
        "searchImagesByUploadedImages(searchByVertexMatchingEngine)"
      ],
      "metadata": {
        "id": "JWH-W9sJELhw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 2.2.9: Cleanup\n",
        "\n",
        "Clean up the resource that's allocated for the Colab, including index, endpoint and the deployment. We need to clean them up in the reverse order, meaning, we first undeploy the index, then delete the index endpoint, and finally delete the index.\n",
        "\n",
        "We are not tracking the completion of the operations here, assuming they will be cleaned up eventually."
      ],
      "metadata": {
        "id": "wFJ-jX25FlzF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "index_endpoint_client.undeploy_index(index_endpoint=INDEX_ENDPOINT_NAME,\n",
        "                                     deployed_index_id=DEPLOYED_INDEX_ID)\n",
        "\n",
        "index_endpoint_client.delete_index_endpoint(name=INDEX_ENDPOINT_NAME)\n",
        "\n",
        "index_client.delete_index(name=INDEX_RESOURCE_NAME)"
      ],
      "metadata": {
        "id": "Imav6fTFM7_l"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}